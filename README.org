* Installing

Run =install.sh=, which will locally create the =pkt_venv= venv and install a
few dependencies.

* Grabbing a node

Perlmutter:

#+begin_src bash
salloc -q interactive -A dune -C cpu -t 240 --ntasks-per-node=256
#+end_src

Cori Haswell:

#+begin_src bash
salloc -q interactive -A dune -C haswell -t 240 --ntasks-per-node=64
#+end_src

Cori KNL:

#+begin_src bash
salloc -q interactive -A dune -C knl -t 240 --ntasks-per-node=272
#+end_src

* Environment setup

#+begin_src bash
source load.sh
#+end_src

Not necessary if you're just submitting batch jobs.

* Interactive launching on a compute node

From a compute node provided by =salloc=:

#+begin_src bash
./packetizer_job.sh /path/to/input.txt
#+end_src

where =input.txt= contains a list of binary files to convert. The output
directory is configured at the top of =packetizer_worker.py=.

* Interactive launching on a login node

This is convenient when there are only a few files to process and/or no urgent
time constraints.

#+begin_src bash
./packetizer_job_local.sh /path/to/input.txt
#+end_src

By default, up to 16 parallel processes will run. This can be increased by
exporting =NPROCS= to something larger, but be considerate.

* Submitting batch jobs

#+begin_src bash
./submit_calibizer.sh /path/to/input.txt [extra sbatch args...]
#+end_src

This calls =sbatch= to submit =packetizer_job.sh=. The latter has some
Perlmutter-specific =SBATCH= directives, so if running on Cori, you will want to
override them when calling =submit_packetizer.sh=.

You can increase the amount of parallelism by asking for more nodes per job
(=-N=) or by asking for more jobs (=--array=).

* Preparing inputs

See the various scripts whose names begin with =dump_input=.
